{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/makaveli10/stylegan3/blob/main/face_merge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Face Morphing - StyleGAN3\n",
        " Network details\n",
        " "
      ],
      "metadata": {
        "id": "zc_u4adNO7Vi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrkC2wSJp2G6"
      },
      "outputs": [],
      "source": [
        "NETWORK = 'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-ffhq-1024x1024.pkl'\n",
        "STEPS = 150\n",
        "FPS = 30\n",
        "FREEZE_STEPS = 30"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title CodeFormer upsampling\n",
        "!git clone https://github.com/sczhou/CodeFormer.git\n",
        "%cd CodeFormer\n",
        "!git checkout e501cd0\n",
        "import sys\n",
        "sys.path.insert(0, \"/content/CodeFormer\")\n",
        "!pip install -r requirements.txt\n",
        "!python basicsr/setup.py develop\n",
        "%cd ..\n",
        "!python CodeFormer/scripts/download_pretrained_models.py facelib\n",
        "!python CodeFormer/scripts/download_pretrained_models.py CodeFormer"
      ],
      "metadata": {
        "id": "fi21tV2REg5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload images"
      ],
      "metadata": {
        "id": "NM5cTpCLPElK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXNu7QDmp8FD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "if len(uploaded) != 1:\n",
        "  print(\"Upload exactly 1 file for source.\")\n",
        "else:\n",
        "  for k, v in uploaded.items():\n",
        "    SOURCE, ext = os.path.splitext(k)\n",
        "    SOURCE_NAME = k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8b2XQrtzp96t"
      },
      "outputs": [],
      "source": [
        "uploaded = files.upload()\n",
        "\n",
        "if len(uploaded) != 1:\n",
        "  print(\"Upload exactly 1 file for target.\")\n",
        "else:\n",
        "  for k, v in uploaded.items():\n",
        "    TARGET, ext = os.path.splitext(k)\n",
        "    TARGET_NAME = k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0P_Nq406qCyT"
      },
      "outputs": [],
      "source": [
        "!wget http://dlib.net/files/shape_predictor_5_face_landmarks.dat.bz2\n",
        "!bzip2 -d shape_predictor_5_face_landmarks.dat.bz2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5EE0c_8qCwA"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "!git clone https://github.com/makaveli10/stylegan3\n",
        "!pip install ninja\n",
        "sys.path.insert(0, \"/content/stylegan3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detect face and crop"
      ],
      "metadata": {
        "id": "6NycrNdjPu9B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bzoj2BIqCtt"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import dlib\n",
        "\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "predictor = dlib.shape_predictor('shape_predictor_5_face_landmarks.dat')\n",
        "\n",
        "def find_eyes(img):\n",
        "  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "  rects = detector(gray, 0)\n",
        "  \n",
        "  if len(rects) == 0:\n",
        "    raise ValueError(\"No faces detected\")\n",
        "  elif len(rects) > 1:\n",
        "    raise ValueError(\"Multiple faces detected\")\n",
        "\n",
        "  shape = predictor(gray, rects[0])\n",
        "  features = []\n",
        "\n",
        "  for i in range(0, 5):\n",
        "    features.append((i, (shape.part(i).x, shape.part(i).y)))\n",
        "\n",
        "  return (int(features[3][1][0] + features[2][1][0]) // 2, \\\n",
        "    int(features[3][1][1] + features[2][1][1]) // 2), \\\n",
        "    (int(features[1][1][0] + features[0][1][0]) // 2, \\\n",
        "    int(features[1][1][1] + features[0][1][1]) // 2)\n",
        "\n",
        "def crop_stylegan(img):\n",
        "  left_eye, right_eye = find_eyes(img)\n",
        "  d = abs(right_eye[0] - left_eye[0])\n",
        "  z = 255/d\n",
        "  ar = img.shape[0]/img.shape[1]\n",
        "  w = img.shape[1] * z\n",
        "  img2 = cv2.resize(img, (int(w), int(w*ar)))\n",
        "  bordersize = 1024\n",
        "  img3 = cv2.copyMakeBorder(\n",
        "      img2,\n",
        "      top=bordersize,\n",
        "      bottom=bordersize,\n",
        "      left=bordersize,\n",
        "      right=bordersize,\n",
        "      borderType=cv2.BORDER_REPLICATE)\n",
        "\n",
        "  left_eye2, right_eye2 = find_eyes(img3)\n",
        "\n",
        "  crop1 = left_eye2[0] - 385 \n",
        "  crop0 = left_eye2[1] - 490\n",
        "  return img3[crop0:crop0+1024,crop1:crop1+1024]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9r3MRj4yqCrR"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "print(SOURCE_NAME)\n",
        "# image_source = cv2.imread(SOURCE_NAME)\n",
        "image_source = cv2.imread(SOURCE_NAME)\n",
        "if image_source is None:\n",
        "    raise ValueError(\"Source image not found\")\n",
        "\n",
        "image_target = cv2.imread(TARGET_NAME)\n",
        "# image_target = cv2.imread(f\"./final_results/{TARGET_NAME}\")\n",
        "if image_target is None:\n",
        "    raise ValueError(\"Source image not found\")\n",
        "\n",
        "cropped_source = crop_stylegan(image_source)\n",
        "cropped_target = crop_stylegan(image_target)\n",
        "\n",
        "img = cv2.cvtColor(cropped_source, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(img)\n",
        "plt.title('source')\n",
        "plt.show()\n",
        "\n",
        "img = cv2.cvtColor(cropped_target, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(img)\n",
        "plt.title('target')\n",
        "plt.show()\n",
        "\n",
        "cv2.imwrite(\"cropped_source.png\", cropped_source)\n",
        "cv2.imwrite(\"cropped_target.png\", cropped_target)\n",
        "\n",
        "#print(find_eyes(cropped_source))\n",
        "#print(find_eyes(cropped_target))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate StyleGAN3 Latents"
      ],
      "metadata": {
        "id": "NBYUJmPYQNCS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rtpbe0zfqCpN"
      },
      "outputs": [],
      "source": [
        "cmd = f\"python /content/stylegan3/projector.py --save-video 0 --num-steps 1000 --outdir={SOURCE} --target=cropped_source.png --network={NETWORK}\"\n",
        "!{cmd}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DF-OZVEUqCnN"
      },
      "outputs": [],
      "source": [
        "cmd = f\"python /content/stylegan3/projector.py --save-video 0 --num-steps 1000 --outdir={TARGET} --target=cropped_target.png --network={NETWORK}\"\n",
        "!{cmd}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYNZrNHc-iNE"
      },
      "outputs": [],
      "source": [
        "img_gan_source = cv2.imread(f'/content/{SOURCE}/proj.png')\n",
        "img = cv2.cvtColor(img_gan_source, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(img)\n",
        "plt.title('source-gan')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3l2f29GqCi9"
      },
      "outputs": [],
      "source": [
        "img_gan_target = cv2.imread(f'/content/{TARGET}/proj.png')\n",
        "img = cv2.cvtColor(img_gan_target, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(img)\n",
        "plt.title('target-gan')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uncomment if codeformer upsampling needed."
      ],
      "metadata": {
        "id": "SIX-JGF8Pgtw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge Faces"
      ],
      "metadata": {
        "id": "5Gm0-PXgQSyA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaIkEET1qCg1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import dnnlib\n",
        "import legacy\n",
        "import PIL.Image\n",
        "import numpy as np\n",
        "import imageio\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "lvec1 = np.load(f'/content/{SOURCE}/projected_w.npz')['w']\n",
        "lvec2 = np.load(f'/content/{TARGET}/projected_w.npz')['w']\n",
        "\n",
        "# network_pkl = \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl\"\n",
        "device = torch.device('cuda')\n",
        "with dnnlib.util.open_url(NETWORK) as fp:\n",
        "    G = legacy.load_network_pkl(fp)['G_ema'].requires_grad_(False).to(device) # type: ignore\n",
        "\n",
        "diff = lvec2 - lvec1\n",
        "step = diff / STEPS\n",
        "current = lvec1.copy()\n",
        "target_uint8 = np.array([1024,1024,3], dtype=np.uint8)\n",
        "\n",
        "video = imageio.get_writer('/content/movie.mp4', mode='I', fps=FPS, codec='libx264', bitrate='16M')\n",
        "\n",
        "for j in tqdm(range(STEPS)):\n",
        "  z = torch.from_numpy(current).to(device)\n",
        "  synth_image = G.synthesis(z, noise_mode='const')\n",
        "  synth_image = (synth_image + 1) * (255/2)\n",
        "  synth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
        "\n",
        "  repeat = FREEZE_STEPS if j==0 or j==(STEPS-1) else 1\n",
        "   \n",
        "  for i in range(repeat):\n",
        "    video.append_data(synth_image)\n",
        "  if j == STEPS/2:\n",
        "    cv2.imwrite(f'./merge_{SOURCE}_{TARGET}.png', cv2.cvtColor(synth_image, cv2.COLOR_RGB2BGR))\n",
        "  current = current + step\n",
        "\n",
        "\n",
        "video.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inp_img = f'./merge_{SOURCE}_{TARGET}.png'\n",
        "cmd = f\"python CodeFormer/inference_codeformer.py -w 0.7 --input_path {inp_img} -o ./\"\n",
        "!{cmd}"
      ],
      "metadata": {
        "id": "ezV4B20qrLTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gv7rgj1MqCeq"
      },
      "outputs": [],
      "source": [
        "download_video = False #@param {type:\"boolean\"}\n",
        "\n",
        "from google.colab import files\n",
        "if download_video: files.download(\"movie.mp4\") "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SAM - Age transformation"
      ],
      "metadata": {
        "id": "IxDGZgZX5GwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content')\n",
        "CODE_DIR = 'SAM'"
      ],
      "metadata": {
        "id": "wrx8hJQQ5GC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/yuval-alaluf/SAM.git $CODE_DIR"
      ],
      "metadata": {
        "id": "YukrtkpV5GAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
        "!sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
        "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force "
      ],
      "metadata": {
        "id": "od5WmsF-5F8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(f'./{CODE_DIR}')"
      ],
      "metadata": {
        "id": "GCBAjvMW5F5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from argparse import Namespace\n",
        "import os\n",
        "import sys\n",
        "import pprint\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "sys.path.append(\".\")\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "from datasets.augmentations import AgeTransformer\n",
        "from utils.common import tensor2im\n",
        "from models.psp import pSp"
      ],
      "metadata": {
        "id": "y42zP4X_5F3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EXPERIMENT_TYPE = 'ffhq_aging'"
      ],
      "metadata": {
        "id": "aNPNEyG35F1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Download Pretrained Model\n",
        "As part of this repository, we provide our pretrained aging model.\n",
        "We'll download the model for the selected experiments as save it to the folder `../pretrained_models`."
      ],
      "metadata": {
        "id": "a7UqVEV55hgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_download_model_command(file_id, file_name):\n",
        "    \"\"\" Get wget download command for downloading the desired model and save to directory ../pretrained_models. \"\"\"\n",
        "    current_directory = os.getcwd()\n",
        "    save_path = os.path.join(os.path.dirname(current_directory), \"pretrained_models\")\n",
        "    if not os.path.exists(save_path):\n",
        "        os.makedirs(save_path)\n",
        "    url = r\"\"\"wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id={FILE_ID}' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id={FILE_ID}\" -O {SAVE_PATH}/{FILE_NAME} && rm -rf /tmp/cookies.txt\"\"\".format(FILE_ID=file_id, FILE_NAME=file_name, SAVE_PATH=save_path)\n",
        "    return url    "
      ],
      "metadata": {
        "id": "mznW_MKN5Fyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_PATHS = {\n",
        "    \"ffhq_aging\": {\"id\": \"1XyumF6_fdAxFmxpFcmPf-q84LU_22EMC\", \"name\": \"sam_ffhq_aging.pt\"}\n",
        "}\n",
        "\n",
        "path = MODEL_PATHS[EXPERIMENT_TYPE]\n",
        "download_command = get_download_model_command(file_id=path[\"id\"], file_name=path[\"name\"]) "
      ],
      "metadata": {
        "id": "OmyJNu4e5ly4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget {download_command}"
      ],
      "metadata": {
        "id": "B2J7PbizIr_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Define Inference Parameters\n",
        "\n",
        "Below we have a dictionary defining parameters such as the path to the pretrained model to use and the path to the\n",
        "image to perform inference on.\n",
        "While we provide default values to run this script, feel free to change as needed."
      ],
      "metadata": {
        "id": "Ps9sJupy6bAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "EXPERIMENT_DATA_ARGS = {\n",
        "    \"ffhq_aging\": {\n",
        "        \"model_path\": \"../pretrained_models/sam_ffhq_aging.pt\",\n",
        "        \"image_path\": SOURCE_NAME,\n",
        "        \"transform\": transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "nzXyBJu15plC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Load Pretrained Model\n",
        "We assume that you have downloaded the pretrained aging model and placed it in the path defined above"
      ],
      "metadata": {
        "id": "DNXtgVrd6hyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EXPERIMENT_ARGS = EXPERIMENT_DATA_ARGS[EXPERIMENT_TYPE]"
      ],
      "metadata": {
        "id": "lKlWmvOGIdZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "4pkeOdRYIi0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = EXPERIMENT_ARGS['model_path']\n",
        "ckpt = torch.load(model_path, map_location='cpu')"
      ],
      "metadata": {
        "id": "BOMcSbAp5pig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opts = ckpt['opts']\n",
        "pprint.pprint(opts)"
      ],
      "metadata": {
        "id": "z6LGO-ao5pfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# update the training options\n",
        "opts['checkpoint_path'] = model_path"
      ],
      "metadata": {
        "id": "kE9HSHqF5pdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opts = Namespace(**opts)\n",
        "net = pSp(opts)\n",
        "net.eval()\n",
        "net.cuda()\n",
        "print('Model successfully loaded!')"
      ],
      "metadata": {
        "id": "77AYJStP5pam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Visualize Input"
      ],
      "metadata": {
        "id": "cv-KXdYK6qfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = EXPERIMENT_DATA_ARGS[EXPERIMENT_TYPE][\"image_path\"]\n",
        "original_image = Image.open(image_path).convert(\"RGB\")"
      ],
      "metadata": {
        "id": "xdIBHB8X6o_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_image.resize((256, 256))"
      ],
      "metadata": {
        "id": "3jHc_gBf6o9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
        "!bzip2 -dk shape_predictor_68_face_landmarks.dat.bz2"
      ],
      "metadata": {
        "id": "9bMlBUGa6o6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_alignment(image_path):\n",
        "    import dlib\n",
        "    from scripts.align_all_parallel import align_face\n",
        "    predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
        "    aligned_image = align_face(filepath=image_path, predictor=predictor) \n",
        "    print(\"Aligned image has shape: {}\".format(aligned_image.size))\n",
        "    return aligned_image "
      ],
      "metadata": {
        "id": "AH0C_4aP60xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aligned_image = run_alignment(image_path)"
      ],
      "metadata": {
        "id": "-pz3HUJV60vW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aligned_image.resize((256, 256))"
      ],
      "metadata": {
        "id": "RV_tmMiK64E_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_transforms = EXPERIMENT_ARGS['transform']\n",
        "input_image = img_transforms(aligned_image)"
      ],
      "metadata": {
        "id": "o3-BFjO_64Cg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we'll run the image on multiple target ages \n",
        "target_ages = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "age_transformers = [AgeTransformer(target_age=age) for age in target_ages]"
      ],
      "metadata": {
        "id": "erCy7plJ64AI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_on_batch(inputs, net):\n",
        "    result_batch = net(inputs.to(\"cuda\").float(), randomize_noise=False, resize=False)\n",
        "    return result_batch"
      ],
      "metadata": {
        "id": "74d-IJwD639s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for each age transformed age, we'll concatenate the results to display them side-by-side\n",
        "os.makedirs('/content/output_age_images')\n",
        "for age_transformer in age_transformers:\n",
        "    print(f\"Running on target age: {age_transformer.target_age}\")\n",
        "    with torch.no_grad():\n",
        "        input_image_age = [age_transformer(input_image.cpu()).to('cuda')]\n",
        "        input_image_age = torch.stack(input_image_age)\n",
        "        result_tensor = run_on_batch(input_image_age, net)[0]\n",
        "        result_image = tensor2im(result_tensor)\n",
        "        result_image.save(\n",
        "            f\"/content/output_age_images/age_transformed_image-{age_transformer.target_age}.jpg\")\n"
      ],
      "metadata": {
        "id": "XgznRTZn69DN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content"
      ],
      "metadata": {
        "id": "d-UYNQEQLhwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title CodeFormer Upsampling\n",
        "results = np.array(aligned_image.resize((1024, 1024)))\n",
        "for age in target_ages:\n",
        "    print(f\"Running on target age: {age}\")\n",
        "    image_path = f\"/content/output_age_images/age_transformed_image-{age}.jpg\"\n",
        "    cmd = f\"python CodeFormer/inference_codeformer.py -w 0.7 --input_path {image_path} -o ./\"\n",
        "    !{cmd}\n",
        "    result_image = Image.open(f'./final_results/age_transformed_image-{age}.png').convert(\"RGB\").resize((1024, 1024))\n",
        "    results = np.concatenate([results, result_image], axis=1)"
      ],
      "metadata": {
        "id": "-tLXHjIvJVDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save image at full resolution\n",
        "final_results = Image.fromarray(results)\n",
        "final_results.save(\"age_transformed_image.jpg\")"
      ],
      "metadata": {
        "id": "nNEXXeOl7AN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J9dZT__u7ALj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4F+OX8ABULii97ppUgd98",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}